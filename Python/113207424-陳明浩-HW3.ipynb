{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f44e500",
   "metadata": {},
   "source": [
    "## ä½œæ¥­ 1: åœŸåœ°æè¿°æ–‡æœ¬åˆ†æ (Strings æ‡‰ç”¨)\n",
    "\n",
    "### é¡Œç›®èªªæ˜\n",
    "è«‹æ’°å¯«ä¸€å€‹ç¨‹å¼ä¾†åˆ†æåœŸåœ°æè¿°æ–‡æœ¬ï¼Œå¾ä¸­æå–é—œéµä¿¡æ¯ï¼Œä¾‹å¦‚åœ°æ®µç·¨è™Ÿã€é¢ç©ã€ä½ç½®å’ŒåœŸåœ°ç”¨é€”åˆ†é¡ç­‰è³‡è¨Šã€‚\n",
    "\n",
    "### è¦æ±‚\n",
    "1. è™•ç†è‡³å°‘ 5 ç­†ä¸åŒçš„åœŸåœ°æè¿°æ–‡æœ¬\n",
    "2. å¾æ¯ç­†æè¿°ä¸­æå–ï¼šåœ°æ®µç·¨è™Ÿã€åŸå¸‚ã€å€åŸŸã€é¢ç©ï¼ˆå«å–®ä½ï¼‰ã€åœŸåœ°ç”¨é€”\n",
    "3. ä½¿ç”¨å­—ä¸²è™•ç†æ–¹æ³•å¦‚ `.split()`ã€`.find()`ã€æ­£å‰‡è¡¨é”å¼ç­‰\n",
    "4. å°‡æå–çš„è³‡è¨Šæ•´ç†æˆçµæ§‹åŒ–çš„æ ¼å¼è¼¸å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_and_extract_string(descriptions):\n",
    "    descriptions_tmp = {\n",
    "        \"åœ°æ®µç·¨è™Ÿ\": [],\n",
    "        \"åŸå¸‚\": [],\n",
    "        \"ç¸½é¢ç©\": [],\n",
    "        \"åœŸåœ°ç”¨é€”\": []\n",
    "    }\n",
    "\n",
    "    for desc in descriptions:\n",
    "        tmp_string = desc.split(\"ï¼Œ\")\n",
    "        descriptions_tmp[\"åœ°æ®µç·¨è™Ÿ\"].append(tmp_string[0].replace(\"åœ°æ®µç·¨è™Ÿ\", \"\"))\n",
    "        descriptions_tmp[\"åŸå¸‚\"].append(tmp_string[1].replace(\"ä½æ–¼\", \"\"))\n",
    "        descriptions_tmp[\"ç¸½é¢ç©\"].append(tmp_string[2].replace(\"ç¸½é¢ç©\", \"\"))\n",
    "        descriptions_tmp[\"åœŸåœ°ç”¨é€”\"].append(tmp_string[3])\n",
    "\n",
    "    return descriptions_tmp\n",
    "\n",
    "# æ¸¬è©¦è³‡æ–™\n",
    "descriptions = [\n",
    "    \"åœ°æ®µç·¨è™ŸA12345ï¼Œä½æ–¼è‡ºåŒ—å¸‚ä¿¡ç¾©å€ï¼Œç¸½é¢ç©250.75å¹³æ–¹å…¬å°ºï¼Œä½å®…ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸZ78910ï¼Œä½æ–¼æ–°åŒ—å¸‚æ¿æ©‹å€ï¼Œç¸½é¢ç©180.30å¹³æ–¹å…¬å°ºï¼Œå•†æ¥­ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸG64026ï¼Œä½æ–¼å°åŒ—å¸‚æ–‡å±±å€ï¼Œç¸½é¢ç©235.70å¹³æ–¹å…¬å°ºï¼Œå·¥æ¥­ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸI19980ï¼Œä½æ–¼æ–°ç«¹ç¸£ç«¹æ±é®ï¼Œç¸½é¢ç©630.35å¹³æ–¹å…¬å°ºï¼Œèƒ½æºç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸU80130ï¼Œä½æ–¼å½°åŒ–ç¸£åŸ”å¿ƒé„‰ï¼Œç¸½é¢ç©990.50å¹³æ–¹å…¬å°ºï¼Œæ•™è‚²ç”¨åœ°\",\n",
    "]\n",
    "\n",
    "# å‘¼å«ä¸¦å°å‡ºçµæœ\n",
    "descriptions_extracted = analize_and_extract_string(descriptions)\n",
    "for i in range(len(descriptions)):\n",
    "    print(f\"åœ°æ®µç·¨è™Ÿ: {descriptions_extracted['åœ°æ®µç·¨è™Ÿ'][i]}\")\n",
    "    print(f\"åŸå¸‚: {descriptions_extracted['åŸå¸‚'][i]}\")\n",
    "    print(f\"ç¸½é¢ç©: {descriptions_extracted['ç¸½é¢ç©'][i]}\")\n",
    "    print(f\"åœŸåœ°ç”¨é€”: {descriptions_extracted['åœŸåœ°ç”¨é€”'][i]}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abf907",
   "metadata": {},
   "source": [
    "## ä½œæ¥­ 2: åœŸåœ°äº¤æ˜“è³‡æ–™åˆ†æ (Lists æ‡‰ç”¨)\n",
    "\n",
    "### é¡Œç›®èªªæ˜\n",
    "è«‹æ’°å¯«ä¸€å€‹ç¨‹å¼ä¾†è™•ç†å’Œåˆ†æåœŸåœ°äº¤æ˜“è³‡æ–™ï¼Œä½¿ç”¨åˆ—è¡¨ï¼ˆListsï¼‰ä¾†å„²å­˜å’Œæ“ä½œäº¤æ˜“è³‡æ–™ï¼Œä¸¦è¨ˆç®—é—œéµçµ±è¨ˆæ•¸æ“šã€‚\n",
    "\n",
    "### è¦æ±‚\n",
    "1. è¨ˆç®—æ¯å€‹å€åŸŸçš„æ¯å¹³æ–¹å…¬å°ºå¹³å‡åƒ¹æ ¼\n",
    "2. æ‰¾å‡ºæœ€è²´å’Œæœ€ä¾¿å®œçš„äº¤æ˜“ç´€éŒ„\n",
    "3. åˆ†æä¸åŒå¹´ä»½çš„åƒ¹æ ¼è¶¨å‹¢\n",
    "4. æ ¹æ“šä¸åŒæ¢ä»¶ç¯©é¸äº¤æ˜“è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def land_transaction_data_analysis(land_transaction_data, analysis_items):\n",
    "    match analysis_items:\n",
    "        case \"å¹³å‡åƒ¹æ ¼\":\n",
    "            average_price = []\n",
    "            for data in land_transaction_data:\n",
    "                average_price.append(data[1] / data[0])  # åƒ¹æ ¼ / é¢ç©\n",
    "            return average_price\n",
    "\n",
    "        case \"æœ€è²´åƒ¹æ ¼\":\n",
    "            highest_price = max(data[1] for data in land_transaction_data)\n",
    "            return highest_price\n",
    "\n",
    "        case \"æœ€ä¾¿å®œåƒ¹æ ¼\":\n",
    "            lowest_price = min(data[1] for data in land_transaction_data)\n",
    "            return lowest_price\n",
    "\n",
    "        case \"åƒ¹æ ¼è¶¨å‹¢\":\n",
    "            yearly_prices = {}\n",
    "            for data in land_transaction_data:\n",
    "                year = data[3]\n",
    "                unit_price = data[1] / data[0]\n",
    "                if year not in yearly_prices:\n",
    "                    yearly_prices[year] = []\n",
    "                yearly_prices[year].append(unit_price)\n",
    "\n",
    "            trend = {}\n",
    "            for year in sorted(yearly_prices):\n",
    "                avg = sum(yearly_prices[year]) / len(yearly_prices[year])\n",
    "                trend[year] = round(avg, 2)\n",
    "\n",
    "            return trend\n",
    "\n",
    "        case _:\n",
    "            return \"ä¸æ”¯æ´çš„åˆ†æé …ç›®\"\n",
    "            \n",
    "\n",
    "transactions = [\n",
    "    (120.5, 7500000, \"ä¸­æ­£å€\", 2023),\n",
    "    (85.3, 5200000, \"å¤§å®‰å€\", 2023),\n",
    "    (150.2, 9800000, \"ä¿¡ç¾©å€\", 2022),\n",
    "    (95.6, 6300000, \"æ¾å±±å€\", 2024),\n",
    "    (110.0, 7000000, \"å…§æ¹–å€\", 2022),\n",
    "    (80.0, 4800000, \"è¬è¯å€\", 2023),\n",
    "    (140.3, 9000000, \"å£«æ—å€\", 2022),\n",
    "    (105.5, 6800000, \"æ–‡å±±å€\", 2023),\n",
    "    (130.7, 8500000, \"åŒ—æŠ•å€\", 2023),\n",
    "    (92.4, 5900000, \"ä¸­å±±å€\", 2024),\n",
    "]\n",
    "\n",
    "print(\"å¹³å‡åƒ¹æ ¼ï¼š\", land_transaction_data_analysis(transactions, \"å¹³å‡åƒ¹æ ¼\"))\n",
    "print(\"æœ€è²´åƒ¹æ ¼ï¼š\", land_transaction_data_analysis(transactions, \"æœ€è²´åƒ¹æ ¼\"))\n",
    "print(\"æœ€ä¾¿å®œåƒ¹æ ¼ï¼š\", land_transaction_data_analysis(transactions, \"æœ€ä¾¿å®œåƒ¹æ ¼\"))\n",
    "print(\"åƒ¹æ ¼è¶¨å‹¢ï¼š\", land_transaction_data_analysis(transactions, \"åƒ¹æ ¼è¶¨å‹¢\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf107ca",
   "metadata": {},
   "source": [
    "## ä½œæ¥­ 3: åœŸåœ°ç™»è¨˜è³‡è¨Šç³»çµ± (Dictionaries æ‡‰ç”¨)\n",
    "\n",
    "### é¡Œç›®èªªæ˜\n",
    "è«‹é–‹ç™¼ä¸€å€‹ä½¿ç”¨å­—å…¸ï¼ˆDictionariesï¼‰ä¾†çµ„ç¹”åœŸåœ°ç™»è¨˜è³‡è¨Šçš„ç¨‹å¼ã€‚æ‚¨å°‡å‰µå»ºä¸€å€‹ç³»çµ±ï¼Œå…¶ä¸­åœ°æ®µç·¨è™Ÿä½œç‚ºéµï¼ˆkeyï¼‰ä¾†å­˜å–è©³ç´°çš„è²¡ç”¢è³‡è¨Šã€‚\n",
    "\n",
    "### è¦æ±‚\n",
    "1. è¨­è¨ˆé©ç•¶çš„å­—å…¸çµæ§‹ä¾†å„²å­˜åœŸåœ°è³‡è¨Š\n",
    "2. å¯¦ç¾æ–°å¢åœ°æ®µè³‡æ–™çš„åŠŸèƒ½\n",
    "3. å¯¦ç¾æ ¹æ“šå„ç¨®æ¢ä»¶æŸ¥è©¢åœŸåœ°è³‡æ–™çš„åŠŸèƒ½\n",
    "4. å¯¦ç¾è¨ˆç®—æ•´å€‹ç™»è¨˜ç³»çµ±çµ±è¨ˆæ•¸æ“šçš„åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537b508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ°æ®µç·¨è™Ÿ: A12345\n",
      "åŸå¸‚: è‡ºåŒ—å¸‚ä¿¡ç¾©å€\n",
      "ç¸½é¢ç©: 250.75å¹³æ–¹å…¬å°º\n",
      "åœŸåœ°ç”¨é€”: ä½å®…ç”¨åœ°\n",
      "------\n",
      "åœ°æ®µç·¨è™Ÿ: Z78910\n",
      "åŸå¸‚: æ–°åŒ—å¸‚æ¿æ©‹å€\n",
      "ç¸½é¢ç©: 180.30å¹³æ–¹å…¬å°º\n",
      "åœŸåœ°ç”¨é€”: å•†æ¥­ç”¨åœ°\n",
      "------\n",
      "åœ°æ®µç·¨è™Ÿ: G64026\n",
      "åŸå¸‚: å°åŒ—å¸‚æ–‡å±±å€\n",
      "ç¸½é¢ç©: 235.70å¹³æ–¹å…¬å°º\n",
      "åœŸåœ°ç”¨é€”: å·¥æ¥­ç”¨åœ°\n",
      "------\n",
      "åœ°æ®µç·¨è™Ÿ: I19980\n",
      "åŸå¸‚: æ–°ç«¹ç¸£ç«¹æ±é®\n",
      "ç¸½é¢ç©: 630.35å¹³æ–¹å…¬å°º\n",
      "åœŸåœ°ç”¨é€”: èƒ½æºç”¨åœ°\n",
      "------\n",
      "åœ°æ®µç·¨è™Ÿ: U80130\n",
      "åŸå¸‚: å½°åŒ–ç¸£åŸ”å¿ƒé„‰\n",
      "ç¸½é¢ç©: 990.50å¹³æ–¹å…¬å°º\n",
      "åœŸåœ°ç”¨é€”: æ•™è‚²ç”¨åœ°\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "def analize_and_extract_string(descriptions):\n",
    "    descriptions_tmp = {\n",
    "        \"åœ°æ®µç·¨è™Ÿ\": [],\n",
    "        \"åŸå¸‚\": [],\n",
    "        \"ç¸½é¢ç©\": [],\n",
    "        \"åœŸåœ°ç”¨é€”\": []\n",
    "    }\n",
    "\n",
    "    for desc in descriptions:\n",
    "        tmp_string = desc.split(\"ï¼Œ\")\n",
    "        descriptions_tmp[\"åœ°æ®µç·¨è™Ÿ\"].append(tmp_string[0].replace(\"åœ°æ®µç·¨è™Ÿ\", \"\"))\n",
    "        descriptions_tmp[\"åŸå¸‚\"].append(tmp_string[1].replace(\"ä½æ–¼\", \"\"))\n",
    "        descriptions_tmp[\"ç¸½é¢ç©\"].append(tmp_string[2].replace(\"ç¸½é¢ç©\", \"\"))\n",
    "        descriptions_tmp[\"åœŸåœ°ç”¨é€”\"].append(tmp_string[3])\n",
    "\n",
    "    return descriptions_tmp\n",
    "\n",
    "# æ¸¬è©¦è³‡æ–™\n",
    "descriptions = [\n",
    "    \"åœ°æ®µç·¨è™ŸA12345ï¼Œä½æ–¼è‡ºåŒ—å¸‚ä¿¡ç¾©å€ï¼Œç¸½é¢ç©250.75å¹³æ–¹å…¬å°ºï¼Œä½å®…ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸZ78910ï¼Œä½æ–¼æ–°åŒ—å¸‚æ¿æ©‹å€ï¼Œç¸½é¢ç©180.30å¹³æ–¹å…¬å°ºï¼Œå•†æ¥­ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸG64026ï¼Œä½æ–¼å°åŒ—å¸‚æ–‡å±±å€ï¼Œç¸½é¢ç©235.70å¹³æ–¹å…¬å°ºï¼Œå·¥æ¥­ç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸI19980ï¼Œä½æ–¼æ–°ç«¹ç¸£ç«¹æ±é®ï¼Œç¸½é¢ç©630.35å¹³æ–¹å…¬å°ºï¼Œèƒ½æºç”¨åœ°\",\n",
    "    \"åœ°æ®µç·¨è™ŸU80130ï¼Œä½æ–¼å½°åŒ–ç¸£åŸ”å¿ƒé„‰ï¼Œç¸½é¢ç©990.50å¹³æ–¹å…¬å°ºï¼Œæ•™è‚²ç”¨åœ°\",\n",
    "]\n",
    "\n",
    "# å‘¼å«ä¸¦å°å‡ºçµæœ\n",
    "descriptions_extracted = analize_and_extract_string(descriptions)\n",
    "for i in range(len(descriptions)):\n",
    "    print(f\"åœ°æ®µç·¨è™Ÿ: {descriptions_extracted['åœ°æ®µç·¨è™Ÿ'][i]}\")\n",
    "    print(f\"åŸå¸‚: {descriptions_extracted['åŸå¸‚'][i]}\")\n",
    "    print(f\"ç¸½é¢ç©: {descriptions_extracted['ç¸½é¢ç©'][i]}\")\n",
    "    print(f\"åœŸåœ°ç”¨é€”: {descriptions_extracted['åœŸåœ°ç”¨é€”'][i]}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94bb3e",
   "metadata": {},
   "source": [
    "## ä½œæ¥­ 4: æˆ¿åœ°ç”¢å¸‚å ´è¶¨å‹¢åˆ†æ (Pandas å’Œ Numpy æ‡‰ç”¨)\n",
    "\n",
    "### é¡Œç›®èªªæ˜\n",
    "ä½¿ç”¨ Pandas å’Œ Numpy ä¾†åˆ†ææˆ¿åœ°ç”¢å¸‚å ´çš„è¶¨å‹¢æ•¸æ“šã€‚æ‚¨å°‡è™•ç†ä¸€å€‹åŒ…å«å¤šå¹´æˆ¿åœ°ç”¢äº¤æ˜“è¨˜éŒ„çš„è³‡æ–™é›†ï¼Œé€²è¡Œæ•¸æ“šæ¸…ç†ã€çµ±è¨ˆåˆ†æå’Œè¦–è¦ºåŒ–ã€‚\n",
    "\n",
    "### è¦æ±‚\n",
    "1. å°å…¥ä¸¦æ¸…ç†æˆ¿åœ°ç”¢äº¤æ˜“è³‡æ–™\n",
    "2. ä½¿ç”¨ Pandas è¨ˆç®—å„å€åŸŸã€å„é¡å‹æˆ¿ç”¢çš„åƒ¹æ ¼çµ±è¨ˆæ•¸æ“š\n",
    "3. ä½¿ç”¨ Numpy é€²è¡Œæ™‚é–“åºåˆ—åˆ†æï¼Œè¨ˆç®—åƒ¹æ ¼è®ŠåŒ–è¶¨å‹¢\n",
    "4. å‰µå»ºæ•¸æ“šè¦–è¦ºåŒ–ä¾†å‘ˆç¾å¸‚å ´è¶¨å‹¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e40b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'æ—¥æœŸ': pd.date_range(start='2018-01-01', periods=100, freq='M'),\n",
    "    'å€åŸŸ': np.random.choice(['ä¸­æ­£å€', 'å¤§å®‰å€', 'ä¿¡ç¾©å€', 'æ¾å±±å€', 'å…§æ¹–å€'], 100),\n",
    "    'é¢ç©': np.random.uniform(20, 200, 100),\n",
    "    'åƒ¹æ ¼': np.random.uniform(5000000, 50000000, 100),\n",
    "    'é¡å‹': np.random.choice(['å…¬å¯“', 'é›»æ¢¯å¤§æ¨“', 'é€å¤©å', 'è¯å»ˆ'], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna(inplace=True)  # è‹¥æœ‰ç¼ºå€¼å°±åˆªé™¤\n",
    "df['å–®åƒ¹'] = df['åƒ¹æ ¼'] / df['é¢ç©']  # å–®åƒ¹ = ç¸½åƒ¹ / é¢ç©\n",
    "region_stats = df.groupby('å€åŸŸ')['åƒ¹æ ¼'].describe()\n",
    "type_stats = df.groupby('é¡å‹')['åƒ¹æ ¼'].describe()\n",
    "print(\"å„å€åŸŸåƒ¹æ ¼çµ±è¨ˆï¼š\")\n",
    "print(region_stats)\n",
    "print(\"\\nå„é¡å‹åƒ¹æ ¼çµ±è¨ˆï¼š\")\n",
    "print(type_stats)\n",
    "df['å¹´æœˆ'] = df['æ—¥æœŸ'].dt.to_period('M')\n",
    "monthly_avg_price = df.groupby('å¹´æœˆ')['å–®åƒ¹'].mean()\n",
    "x = np.arange(len(monthly_avg_price))\n",
    "y = monthly_avg_price.values\n",
    "z = np.polyfit(x, y, 1)  # ä¸€éšå¤šé …å¼ï¼ˆç·šæ€§ï¼‰\n",
    "p = np.poly1d(z)         # è¶¨å‹¢ç·šæ–¹ç¨‹å¼\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_avg_price.index.astype(str), monthly_avg_price.values, label='æ¯æœˆå¹³å‡å–®åƒ¹')\n",
    "plt.plot(monthly_avg_price.index.astype(str), p(x), label='åƒ¹æ ¼è¶¨å‹¢ç·š', linestyle='--')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('å¹´æœˆ')\n",
    "plt.ylabel('å¹³å‡å–®åƒ¹ï¼ˆå…ƒ/åªï¼‰')\n",
    "plt.title('æˆ¿åœ°ç”¢å¸‚å ´åƒ¹æ ¼è¶¨å‹¢åˆ†æ')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ba8c4",
   "metadata": {},
   "source": [
    "## ä½œæ¥­ 5: åœŸåœ°åƒ¹æ ¼é æ¸¬æ¨¡å‹ (scikit-learn æ‡‰ç”¨)\n",
    "\n",
    "### é¡Œç›®èªªæ˜\n",
    "ä½¿ç”¨ scikit-learn å»ºç«‹ä¸€å€‹é æ¸¬åœŸåœ°åƒ¹æ ¼çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ã€‚æ‚¨å°‡ä½¿ç”¨æ­·å²äº¤æ˜“æ•¸æ“šä¾†è¨“ç·´æ¨¡å‹ï¼Œä¸¦è©•ä¼°å…¶é æ¸¬æº–ç¢ºæ€§ã€‚\n",
    "\n",
    "### è¦æ±‚\n",
    "1. æº–å‚™ä¸¦é è™•ç†åœŸåœ°äº¤æ˜“è³‡æ–™é›†\n",
    "2. é¸æ“‡é©ç•¶çš„ç‰¹å¾µä¸¦é€²è¡Œç‰¹å¾µå·¥ç¨‹\n",
    "3. ä½¿ç”¨ scikit-learn è¨“ç·´è‡³å°‘å…©ç¨®ä¸åŒçš„æ¨¡å‹ (å¦‚ç·šæ€§è¿´æ­¸ã€éš¨æ©Ÿæ£®æ—ç­‰)\n",
    "4. è©•ä¼°ä¸¦æ¯”è¼ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½\n",
    "5. ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œåƒ¹æ ¼é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a31f77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ==== æ¨¡æ“¬è³‡æ–™ ====\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "data = {\n",
    "    'é¢ç©': np.random.uniform(20, 500, n_samples),\n",
    "    'è·é›¢å¸‚ä¸­å¿ƒ': np.random.uniform(0, 30, n_samples),\n",
    "    'åœ°å½¢': np.random.choice(['å¹³åœ°', 'å¡åœ°', 'å±±å¡åœ°'], n_samples),\n",
    "    'ç”¨é€”åˆ†å€': np.random.choice(['ä½å®…å€', 'å•†æ¥­å€', 'å·¥æ¥­å€', 'è¾²æ¥­å€'], n_samples),\n",
    "    'è‡¨è·¯å¯¬åº¦': np.random.uniform(2, 20, n_samples),\n",
    "    'åƒ¹æ ¼': None\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['åƒ¹æ ¼'] = (df['é¢ç©'] * 100000 +\n",
    "              df['è‡¨è·¯å¯¬åº¦'] * 50000 -\n",
    "              df['è·é›¢å¸‚ä¸­å¿ƒ'] * 100000 +\n",
    "              np.random.normal(0, 1000000, n_samples))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['åœ°å½¢'] == 'å¹³åœ°':\n",
    "        df.at[idx, 'åƒ¹æ ¼'] *= 1.2\n",
    "    elif row['åœ°å½¢'] == 'å¡åœ°':\n",
    "        df.at[idx, 'åƒ¹æ ¼'] *= 0.9\n",
    "\n",
    "    if row['ç”¨é€”åˆ†å€'] == 'å•†æ¥­å€':\n",
    "        df.at[idx, 'åƒ¹æ ¼'] *= 1.5\n",
    "    elif row['ç”¨é€”åˆ†å€'] == 'å·¥æ¥­å€':\n",
    "        df.at[idx, 'åƒ¹æ ¼'] *= 1.3\n",
    "    elif row['ç”¨é€”åˆ†å€'] == 'è¾²æ¥­å€':\n",
    "        df.at[idx, 'åƒ¹æ ¼'] *= 0.7\n",
    "\n",
    "# ==== åˆ†å‰²è³‡æ–™ ====\n",
    "X = df.drop('åƒ¹æ ¼', axis=1)\n",
    "y = df['åƒ¹æ ¼']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==== é è™•ç†å™¨ ====\n",
    "numeric_features = ['é¢ç©', 'è·é›¢å¸‚ä¸­å¿ƒ', 'è‡¨è·¯å¯¬åº¦']\n",
    "categorical_features = ['åœ°å½¢', 'ç”¨é€”åˆ†å€']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==== å»ºç«‹å…©ç¨®æ¨¡å‹ ====\n",
    "models = {\n",
    "    \"ç·šæ€§è¿´æ­¸\": Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \"éš¨æ©Ÿæ£®æ—\": Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ==== æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ ====\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R2': r2}\n",
    "    print(f\"\\næ¨¡å‹ï¼š{name}\")\n",
    "    print(f\"â†’ RMSE: {rmse:,.2f}\")\n",
    "    print(f\"â†’ RÂ²: {r2:.4f}\")\n",
    "\n",
    "# ==== ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬ ====\n",
    "best_model_name = max(results, key=lambda x: results[x]['R2'])\n",
    "best_model = models[best_model_name]\n",
    "sample = pd.DataFrame({\n",
    "    'é¢ç©': [150],\n",
    "    'è·é›¢å¸‚ä¸­å¿ƒ': [10],\n",
    "    'åœ°å½¢': ['å¹³åœ°'],\n",
    "    'ç”¨é€”åˆ†å€': ['å•†æ¥­å€'],\n",
    "    'è‡¨è·¯å¯¬åº¦': [12]\n",
    "})\n",
    "predicted_price = best_model.predict(sample)[0]\n",
    "print(f\"\\nâœ… æœ€ä½³æ¨¡å‹ï¼š{best_model_name}\")\n",
    "print(f\"ğŸ  é æ¸¬åƒ¹æ ¼ï¼ˆç¤ºä¾‹åœŸåœ°ï¼‰: {predicted_price:,.2f} å…ƒ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
